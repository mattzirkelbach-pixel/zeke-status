#!/usr/bin/env python3
"""
zeke-financial-ingestion.py — Block 5: Financial Data Ingestion
Sources: CME FedWatch, MacroAlf RSS, Barchart GLD/SLV options, TreasuryDirect auctions
Output: learning-feed.jsonl + spark-work-queue.jsonl follow-up tasks
"""

import json, datetime, urllib.request, urllib.error, html, re, sys
from pathlib import Path

HOME = Path.home()
FEED = HOME / ".openclaw/workspace/memory/learning-feed.jsonl"
QUEUE = HOME / ".openclaw/workspace/memory/spark-work-queue.jsonl"
LOG = HOME / "logs/financial-ingestion.log"

def now_iso():
    return datetime.datetime.now(datetime.timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")

def log(msg):
    ts = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    line = f"[{ts}] {msg}"
    print(line, flush=True)
    LOG.parent.mkdir(exist_ok=True)
    with open(LOG, "a") as f:
        f.write(line + "\n")

def fetch_url(url, timeout=15):
    try:
        req = urllib.request.Request(url, headers={"User-Agent": "Mozilla/5.0"})
        with urllib.request.urlopen(req, timeout=timeout) as r:
            return r.read().decode("utf-8", errors="replace")
    except Exception as e:
        log(f"  WARN fetch {url[:60]}: {e}")
        return None

def write_feed(entry):
    entry["timestamp"] = now_iso()
    with open(FEED, "a") as f:
        f.write(json.dumps(entry) + "\n")

def add_queue_task(label, prompt, priority=6, instrument="GENERAL"):
    try:
        tasks = json.loads(QUEUE.read_text()) if QUEUE.exists() else []
        if any(t.get("label") == label for t in tasks):
            return False
        tasks.append({
            "id": f"fin_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}",
            "priority": priority, "task_type": "research",
            "instrument": instrument, "label": label,
            "description": prompt, "prompt_template": prompt,
            "source": "financial-ingestion", "context_refs": [],
            "created_at": now_iso(), "status": "pending",
            "completed_at": None, "output_path": None
        })
        QUEUE.write_text(json.dumps(tasks, indent=2))
        return True
    except Exception as e:
        log(f"  WARN queue: {e}")
        return False

def ingest_fedwatch():
    log("[1/4] CME FedWatch...")
    # Try multiple FedWatch endpoints
    urls_to_try = [
        "https://www.cmegroup.com/CmeWS/mvc/ProductCalendar/V2/FedWatch",
        "https://www.cmegroup.com/CmeWS/mvc/FedWatch/tool/0/chartdata.json",
        "https://www.cmegroup.com/CmeWS/mvc/FedWatch/tool/0/2026-03.json",
    ]
    text = None
    url = urls_to_try[0]
    for u in urls_to_try:
        text = fetch_url(u, timeout=15)
        if text:
            url = u
            break
    finding = "FedWatch: API unavailable — queue Spark research task"
    if text:
        try:
            data = json.loads(text)
            meetings = data.get("meetings", data.get("data", []))
            if meetings:
                parts = []
                for m in meetings[:3]:
                    d = m.get("meetingDate", m.get("date", "?"))
                    nc = m.get("probNoChange", "?")
                    c25 = m.get("prob25bpsCut", "?")
                    parts.append(f"{d}: hold={nc}% cut25={c25}%")
                finding = "FedWatch: " + " | ".join(parts)
            else:
                finding = f"FedWatch API returned {len(text)} chars but no meetings array parsed"
        except:
            finding = f"FedWatch: page fetched ({len(text)} chars) but JSON parse failed — JS rendering likely required"
    log(f"  → {finding[:100]}")
    write_feed({"topic": "fedwatch-rate-probabilities", "domain": "treasury-bonds",
                "finding": finding, "source": "cme-fedwatch", "instrument": "TLT"})
    add_queue_task("FedWatch rate cut probability — TLT position impact",
        "What is CME FedWatch currently pricing for 2026 rate cuts? How many 25bp cuts are implied by Fed funds futures? Does this support TLT $90-101C strikes for Jan 2027? What yield level is breakeven for each strike?",
        priority=6, instrument="TLT")
    return finding

def ingest_macroalf():
    log("[2/4] MacroAlf RSS (The Macro Compass)...")
    url = "https://themacrocompass.substack.com/feed"
    text = fetch_url(url, timeout=15)
    count = 0
    if text:
        items = re.findall(r'<item>(.*?)</item>', text, re.DOTALL)
        for item in items[:4]:
            tm = re.search(r'<title>(?:<!\[CDATA\[)?(.*?)(?:\]\]>)?</title>', item)
            dm = re.search(r'<description>(?:<!\[CDATA\[)?(.*?)(?:\]\]>)?</description>', item, re.DOTALL)
            pm = re.search(r'<pubDate>(.*?)</pubDate>', item)
            title = html.unescape(tm.group(1).strip()) if tm else "?"
            desc = re.sub(r'<[^>]+>', ' ', dm.group(1) if dm else "").strip()[:350]
            desc = html.unescape(desc)
            pub = pm.group(1).strip() if pm else ""
            if len(title) > 5:
                write_feed({"topic": "macroalf-commentary", "domain": "treasury-bonds",
                            "finding": f"{title} ({pub}): {desc}", "source": "macroalf-rss",
                            "instrument": "MACRO", "url": url})
                count += 1
        if count:
            add_queue_task(f"MacroAlf analysis — macro implications for gold/TLT",
                "Based on recent MacroAlf (The Macro Compass) commentary: (1) What is current global liquidity cycle read? (2) Dollar direction thesis? (3) How does this align with Camel Finance gold bull + TLT bull thesis?",
                priority=6, instrument="MACRO")
    if not count:
        write_feed({"topic": "macroalf-commentary", "domain": "treasury-bonds",
                    "finding": "MacroAlf RSS: no articles parsed (format may have changed)", "source": "macroalf-rss", "instrument": "MACRO"})
    log(f"  → {count} MacroAlf articles ingested")
    return count

def ingest_options_flow(ticker):
    log(f"[3/4] Options flow: {ticker}...")
    # Yahoo Finance v8 API for options chain
    url = f"https://query1.finance.yahoo.com/v7/finance/options/{ticker}"
    text = fetch_url(url, timeout=20)
    finding = f"{ticker} options: fetch failed"
    if text:
        try:
            data = json.loads(text)
            chain = data.get("optionChain", {}).get("result", [{}])[0]
            meta = chain.get("quote", {})
            calls = chain.get("options", [{}])[0].get("calls", [])
            puts = chain.get("options", [{}])[0].get("puts", [])
            iv = meta.get("impliedVolatility", meta.get("twoHundredDayAverageChangePercent", "?"))
            price = meta.get("regularMarketPrice", "?")
            top_call = max(calls, key=lambda x: x.get("openInterest", 0), default={})
            top_put = max(puts, key=lambda x: x.get("openInterest", 0), default={})
            parts = [f"price={price}"]
            if top_call:
                parts.append(f"max-OI-call={top_call.get('strike','?')} exp={top_call.get('expiration','?')} OI={top_call.get('openInterest','?')}")
            if top_put:
                parts.append(f"max-OI-put={top_put.get('strike','?')} OI={top_put.get('openInterest','?')}")
            finding = f"{ticker} options: {' | '.join(parts)}"
        except Exception as e:
            finding = f"{ticker} options: fetched but parse error — {e}"
    log(f"  → {finding[:100]}")
    write_feed({"topic": f"options-flow-{ticker.lower()}", "domain": "camel-finance",
                "finding": finding, "source": f"yahoo-options-{ticker.lower()}",
                "instrument": ticker, "url": url})
    add_queue_task(f"{ticker} options positioning — smart money flow analysis",
        f"Research current {ticker} options market: (1) Put/call ratio, (2) Largest open interest strikes and expiries, (3) IV skew (elevated calls = bullish bet, elevated puts = hedge), (4) Any evidence of unusual block trades. Interpret for current gold/silver cycle position.",
        priority=5, instrument=ticker)
    return finding

def ingest_treasury_calendar():
    log("[4/4] TreasuryDirect auction calendar...")
    url = "https://www.treasurydirect.gov/TA_WS/securities/upcoming"
    text = fetch_url(url, timeout=15)
    finding = "TreasuryDirect: unavailable"
    if text:
        try:
            auctions = json.loads(text)
            if isinstance(auctions, list) and auctions:
                key = [a for a in auctions if any(t in str(a.get("securityTerm","")) for t in ["10-Year","30-Year","7-Year","20-Year","5-Year","2-Year"])]
                show = key[:4] if key else auctions[:4]
                parts = [f"{a.get('securityTerm','?')} on {a.get('auctionDate','?')} (${a.get('offeringAmount','?')}B)" for a in show]
                finding = "Upcoming auctions: " + " | ".join(parts)
        except:
            dates = re.findall(r'\d{4}-\d{2}-\d{2}', text[:2000])
            finding = f"Treasury calendar: {len(dates)} dates found — {', '.join(dates[:5])}"
    log(f"  → {finding[:100]}")
    write_feed({"topic": "treasury-auction-calendar", "domain": "treasury-bonds",
                "finding": finding, "source": "treasurydirect", "instrument": "TLT", "url": url})
    if "auction" in finding and "unavailable" not in finding:
        add_queue_task("Treasury auction demand — TLT directional impact",
            f"Upcoming Treasury auctions: {finding}. Analyze: (1) Which auctions most impact TLT? (2) Strong demand = lower yields = TLT up. What bid-to-cover signals strength? (3) Any auctions before our key TLT decision dates?",
            priority=6, instrument="TLT")
    return finding

def main():
    log("=" * 60)
    log("zeke-financial-ingestion.py — Block 5")
    log("=" * 60)
    ingest_fedwatch()
    ingest_macroalf()
    ingest_options_flow("GLD")
    ingest_options_flow("SLV")
    ingest_treasury_calendar()
    log("Done — feed updated, queue tasks added")
    log("=" * 60)

if __name__ == "__main__":
    main()
